{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9debf8a1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42787e0b",
   "metadata": {},
   "source": [
    "In this project, we used Reinforcement Learning to train an agent to complete a simplified version of our assigment in Project 1 of this class: the game Exactly One Mazes.\n",
    "\n",
    "This agent will be trained in order for it to learn how to win the game.\n",
    "\n",
    "The objective of the game is to have the player cross from the bottom-left to the top right square, without crossing the same L shape more than one time. The player also must not navigate sections of the board he has already visited.\n",
    "\n",
    "The Player can move in any direction, as long as the square he is in does not belong to an already visited L shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b517b",
   "metadata": {},
   "source": [
    "# Required Libraries and Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105f11e",
   "metadata": {},
   "source": [
    "● OpenAI Gym - A framework that acts as a playground for testing agents, using\n",
    "controlled environments;\n",
    "\n",
    "● Numpy - A library for processing data in arrays;\n",
    "\n",
    "● Matplotlib - Data visualization libraries to draw plots and charts;\n",
    "\n",
    "● Jupyter Notebooks - Interactive computing and development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b31e8d",
   "metadata": {},
   "source": [
    "# Algorithms used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bd96a",
   "metadata": {},
   "source": [
    "To teach the agent, the following algorithms were implemented:\n",
    "\n",
    "● Q-Learning - State–action–reward–state;\n",
    "\n",
    "● SARSA - State–action–reward–state–action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed71073",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c1650",
   "metadata": {},
   "source": [
    "In this notebook, we are using two models.\n",
    "\n",
    "Both are 4x4 boards, the simpler one having two L shapes, with the second, more complex one having three.\n",
    "\n",
    "The rewards were distributed in such a way that the algorithm was severely punished for every bad move:\n",
    "\n",
    "● If visiting an already visited cell, -50 points\n",
    "\n",
    "● If visiting an L shape for the second time, -50 points\n",
    "\n",
    "● If it reached the end without visiting all L shapes, -50 points\n",
    "\n",
    "● If landing in a neutral (0) cell, -0.50 points (-0.75 in the case of the more complex model)\n",
    "\n",
    "    ● This was made so the algorithm would reach the end as fast as possible\n",
    "\n",
    "● If visiting a newly discovered L shape, +10 points\n",
    "\n",
    "● If it reached the end and visited all L shapes, +10 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3516fb6",
   "metadata": {},
   "source": [
    "# Importing what's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from env import TakeTheLEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f2d6d",
   "metadata": {},
   "source": [
    "# Defining the hyperparameters and creating the Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 500  # the total number of episodes to run\n",
    "max_steps = 100  # the maximum number of steps per episode\n",
    "\n",
    "learning_rate = 0.5\n",
    "gamma = 0.6  # the discount factor\n",
    "\n",
    "# the range for the exploration parameter epsilon\n",
    "epsilon = 0.1\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.01\n",
    "\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5812905",
   "metadata": {},
   "source": [
    "# Defining the learning process and the updating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "    if exp_exp_tradeoff < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(qtable[state, :])\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_qlearning(state, new_state, reward, action):\n",
    "    qtable[state, action] = qtable[state, action] + learning_rate * (\n",
    "        reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action]\n",
    "    )\n",
    "\n",
    "\n",
    "def update_sarsa(state, new_state, reward, action, new_action):\n",
    "    qtable[state, action] = qtable[state, action] + learning_rate * (\n",
    "        reward + gamma * qtable[new_state, new_action] - qtable[state, action]\n",
    "    )\n",
    "\n",
    "# To keep track of the rewards and the epsilon state over the episodes\n",
    "rewards = []\n",
    "epsilons = []\n",
    "\n",
    "\n",
    "def learn(use_sarsa):\n",
    "    global qtable\n",
    "    success = 0\n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        for _ in range(max_steps):\n",
    "            # Converting the state to a position on the table\n",
    "            action = choose_action(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if use_sarsa:\n",
    "                new_action = choose_action(new_state)\n",
    "                update_sarsa(state, new_state, reward, action, new_action)\n",
    "            else:\n",
    "                update_qlearning(state, new_state, reward, action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            state = new_state\n",
    "            env.set_state(env.from_idx(new_state))\n",
    "\n",
    "            if done:\n",
    "                if total_rewards < 0:\n",
    "                    print(\"Failed episode:\", episode)\n",
    "                else:\n",
    "                    env.render()\n",
    "                    success += 1\n",
    "                print(\"Total reward for episode {}: {}\".format(episode, total_rewards))\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(\n",
    "            -decay_rate * episode\n",
    "        )\n",
    "        rewards.append(total_rewards)\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "    print(\"Score/time: \" + str(sum(rewards) / total_episodes))\n",
    "    print(\"success of\", success/total_episodes)\n",
    "    print(qtable)\n",
    "\n",
    "    x = range(total_episodes)\n",
    "    plt.plot(x, rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Training total reward\")\n",
    "    plt.title(\"Total rewards over all episodes in training\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epsilons)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.title(\"Epsilon for episode\")\n",
    "    plt.show()\n",
    "\n",
    "    qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards.clear()\n",
    "    epsilons.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a455f",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b4f6f",
   "metadata": {},
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9024c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TakeTheLEnv()\n",
    "env.set_board( [[1, 2, 2, 0],\n",
    "                [1, 0, 2, 0],\n",
    "                [1, 1, 2, 0],\n",
    "                [0, 0, 0, 0]])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f636f6",
   "metadata": {},
   "source": [
    "## Algorithms execution for the simple board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d674a90",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc769e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(use_sarsa=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59065dc",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(use_sarsa=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e11c8",
   "metadata": {},
   "source": [
    "#### Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TakeTheLEnv()\n",
    "env.set_board( [[1, 3, 3, 0],\n",
    "                [1, 0, 3, 2],\n",
    "                [1, 1, 3, 2],\n",
    "                [0, 0, 2, 2]])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fa09e",
   "metadata": {},
   "source": [
    "## Algorithms execution for the complex board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d290ff",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786febbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(use_sarsa=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8ae17",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6520ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(use_sarsa=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
